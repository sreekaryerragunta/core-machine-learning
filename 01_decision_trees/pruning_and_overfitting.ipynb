{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pruning and Overfitting\n",
                "\n",
                "Understanding and preventing overfitting in decision trees:\n",
                "1. Demonstrating overfitting\n",
                "2. Pre-pruning (early stopping)\n",
                "3. Analyzing hyperparameter effects\n",
                "4. Learning curves\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import make_classification, make_moons\n",
                "from sklearn.model_selection import train_test_split, learning_curve\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Demonstrating Overfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate noisy dataset\n",
                "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0,\n",
                "                          n_informative=2, n_clusters_per_class=1,\n",
                "                          flip_y=0.1, random_state=42)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42\n",
                ")\n",
                "\n",
                "print(f'Training samples: {len(X_train)}')\n",
                "print(f'Test samples: {len(X_test)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare Shallow vs Deep Trees"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train trees with different depths\n",
                "depths = [2, 5, None]  # None = unlimited depth\n",
                "titles = ['Max Depth = 2 (Underfit)', 'Max Depth = 5 (Good Fit)', 'Unlimited Depth (Overfit)']\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "\n",
                "for idx, (depth, title) in enumerate(zip(depths, titles)):\n",
                "    # Train tree\n",
                "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
                "    tree.fit(X_train, y_train)\n",
                "    \n",
                "    # Calculate accuracies\n",
                "    train_acc = tree.score(X_train, y_train)\n",
                "    test_acc = tree.score(X_test, y_test)\n",
                "    \n",
                "    # Create mesh for decision boundary\n",
                "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
                "                         np.arange(y_min, y_max, 0.02))\n",
                "    \n",
                "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "    \n",
                "    # Plot\n",
                "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
                "    axes[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=40,\n",
                "                     edgecolors='k', cmap='RdYlBu', alpha=0.7, label='Train')\n",
                "    axes[idx].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=40,\n",
                "                     edgecolors='yellow', linewidths=2, cmap='RdYlBu', \n",
                "                     alpha=0.7, marker='s', label='Test')\n",
                "    \n",
                "    axes[idx].set_title(f'{title}\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}\\n' +\n",
                "                       f'Depth: {tree.get_depth()}, Leaves: {tree.get_n_leaves()}',\n",
                "                       fontsize=11, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Feature 1')\n",
                "    axes[idx].set_ylabel('Feature 2')\n",
                "    axes[idx].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Observation:\n",
                "- **Depth = 2**: Simple boundary, underfits (low train/test accuracy)\n",
                "- **Depth = 5**: Balanced complexity, good generalization\n",
                "- **Unlimited depth**: Complex jagged boundary, overfits (train >> test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Effect of Hyperparameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Max Depth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "depths_range = range(1, 21)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "n_leaves = []\n",
                "\n",
                "for depth in depths_range:\n",
                "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
                "    tree.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores.append(tree.score(X_train, y_train))\n",
                "    test_scores.append(tree.score(X_test, y_test))\n",
                "    n_leaves.append(tree.get_n_leaves())\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Accuracy vs depth\n",
                "axes[0].plot(depths_range, train_scores, 'o-', label='Train Accuracy', linewidth=2)\n",
                "axes[0].plot(depths_range, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
                "axes[0].axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
                "axes[0].set_xlabel('Max Depth', fontsize=12)\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Accuracy vs Max Depth', fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Number of leaves\n",
                "axes[1].plot(depths_range, n_leaves, 'o-', linewidth=2, color='green')\n",
                "axes[1].set_xlabel('Max Depth', fontsize=12)\n",
                "axes[1].set_ylabel('Number of Leaf Nodes', fontsize=12)\n",
                "axes[1].set_title('Tree Complexity vs Max Depth', fontsize=13, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "optimal_depth = depths_range[np.argmax(test_scores)]\n",
                "print(f'Optimal max_depth: {optimal_depth}')\n",
                "print(f'Test accuracy at optimal: {max(test_scores):.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Min Samples Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "min_samples_range = [2, 5, 10, 20, 50, 100]\n",
                "train_scores_split = []\n",
                "test_scores_split = []\n",
                "\n",
                "for min_samples in min_samples_range:\n",
                "    tree = DecisionTreeClassifier(min_samples_split=min_samples, random_state=42)\n",
                "    tree.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores_split.append(tree.score(X_train, y_train))\n",
                "    test_scores_split.append(tree.score(X_test, y_test))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(min_samples_range, train_scores_split, 'o-', label='Train', linewidth=2)\n",
                "plt.plot(min_samples_range, test_scores_split, 's-', label='Test', linewidth=2)\n",
                "plt.xlabel('Min Samples Split', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('Effect of Min Samples Split', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print('Higher min_samples_split → simpler tree → less overfitting')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Min Samples Leaf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "min_leaf_range = [1, 5, 10, 20, 30]\n",
                "train_scores_leaf = []\n",
                "test_scores_leaf = []\n",
                "\n",
                "for min_leaf in min_leaf_range:\n",
                "    tree = DecisionTreeClassifier(min_samples_leaf=min_leaf, random_state=42)\n",
                "    tree.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores_leaf.append(tree.score(X_train, y_train))\n",
                "    test_scores_leaf.append(tree.score(X_test, y_test))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(min_leaf_range, train_scores_leaf, 'o-', label='Train', linewidth=2)\n",
                "plt.plot(min_leaf_range, test_scores_leaf, 's-', label='Test', linewidth=2)\n",
                "plt.xlabel('Min Samples Leaf', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('Effect of Min Samples Leaf', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Learning Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate larger dataset\n",
                "X_large, y_large = make_classification(n_samples=1000, n_features=20, \n",
                "                                      n_informative=15, n_redundant=5,\n",
                "                                      random_state=42)\n",
                "\n",
                "# Compare shallow vs deep tree\n",
                "tree_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "tree_deep = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
                "\n",
                "# Compute learning curves\n",
                "train_sizes = np.linspace(0.1, 1.0, 10)\n",
                "\n",
                "train_sizes_shallow, train_scores_shallow, val_scores_shallow = learning_curve(\n",
                "    tree_shallow, X_large, y_large, train_sizes=train_sizes, cv=5, random_state=42\n",
                ")\n",
                "\n",
                "train_sizes_deep, train_scores_deep, val_scores_deep = learning_curve(\n",
                "    tree_deep, X_large, y_large, train_sizes=train_sizes, cv=5, random_state=42\n",
                ")\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "#Shallow tree\n",
                "axes[0].plot(train_sizes_shallow, train_scores_shallow.mean(axis=1), \n",
                "            'o-', label='Train', linewidth=2)\n",
                "axes[0].plot(train_sizes_shallow, val_scores_shallow.mean(axis=1),\n",
                "            's-', label='Validation', linewidth=2)\n",
                "axes[0].fill_between(train_sizes_shallow, \n",
                "                    train_scores_shallow.mean(axis=1) - train_scores_shallow.std(axis=1),\n",
                "                    train_scores_shallow.mean(axis=1) + train_scores_shallow.std(axis=1),\n",
                "                    alpha=0.2)\n",
                "axes[0].fill_between(train_sizes_shallow,\n",
                "                    val_scores_shallow.mean(axis=1) - val_scores_shallow.std(axis=1),\n",
                "                    val_scores_shallow.mean(axis=1) + val_scores_shallow.std(axis=1),\n",
                "                    alpha=0.2)\n",
                "axes[0].set_xlabel('Training Size', fontsize=12)\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Learning Curve: Shallow Tree (max_depth=3)', \n",
                "                 fontsize=13, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Deep tree\n",
                "axes[1].plot(train_sizes_deep, train_scores_deep.mean(axis=1),\n",
                "            'o-', label='Train', linewidth=2)\n",
                "axes[1].plot(train_sizes_deep, val_scores_deep.mean(axis=1),\n",
                "            's-', label='Validation', linewidth=2)\n",
                "axes[1].fill_between(train_sizes_deep,\n",
                "                    train_scores_deep.mean(axis=1) - train_scores_deep.std(axis=1),\n",
                "                    train_scores_deep.mean(axis=1) + train_scores_deep.std(axis=1),\n",
                "                    alpha=0.2)\n",
                "axes[1].fill_between(train_sizes_deep,\n",
                "                    val_scores_deep.mean(axis=1) - val_scores_deep.std(axis=1),\n",
                "                    val_scores_deep.mean(axis=1) + val_scores_deep.std(axis=1),\n",
                "                    alpha=0.2)\n",
                "axes[1].set_xlabel('Training Size', fontsize=12)\n",
                "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[1].set_title('Learning Curve: Deep Tree (unlimited depth)',\n",
                "                 fontsize=13, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Interpretation:\n",
                "- **Shallow tree**: Small gap between train/val → high bias, low variance\n",
                "- **Deep tree**: Large gap between train/val → low bias, high variance (overfitting)\n",
                "- **More data helps** reduce overfitting for deep trees"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: Pre-Pruning Best Practices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test multiple hyperparameter combinations\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "param_grid = {\n",
                "    'max_depth': [3, 5, 7, 10, None],\n",
                "    'min_samples_split': [2, 10, 20],\n",
                "    'min_samples_leaf': [1, 5, 10],\n",
                "    'criterion': ['gini', 'entropy']\n",
                "}\n",
                "\n",
                "tree = DecisionTreeClassifier(random_state=42)\n",
                "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy')\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print('='*60)\n",
                "print('GRID SEARCH RESULTS')\n",
                "print('='*60)\n",
                "print(f'Best parameters: {grid_search.best_params_}')\n",
                "print(f'Best CV score: {grid_search.best_score_:.4f}')\n",
                "print(f'Test score: {grid_search.score(X_test, y_test):.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Overfitting Signs:\n",
                "1. Train accuracy >> Test accuracy\n",
                "2. Very deep trees (> 10 levels)\n",
                "3. Many leaf nodes\n",
                "4. Complex, jagged decision boundaries\n",
                "\n",
                "### Prevention Strategies:\n",
                "\n",
                "#### Pre-Pruning (Early Stopping):\n",
                "- **max_depth**: Limit tree depth (try 3-10)\n",
                "- **min_samples_split**: Min samples to split (try 10-50)\n",
                "- **min_samples_leaf**: Min samples per leaf (try 5-20)\n",
                "- **max_leaf_nodes**: Limit total leaves\n",
                "- **min_impurity_decrease**: Min improvement to split\n",
                "\n",
                "#### Other Approaches:\n",
                "- **More training data**: Helps deep trees generalize\n",
                "- **Feature selection**: Remove irrelevant features\n",
                "- **Ensemble methods**: Use Random Forests instead\n",
                "- **Cross-validation**: Always validate hyperparameters\n",
                "\n",
                "### Tuning Workflow:\n",
                "1. Start with shallow tree (depth 3-5)\n",
                "2. Check train vs test performance\n",
                "3. If underfitting → increase complexity\n",
                "4. If overfitting → add constraints\n",
                "5. Use GridSearchCV for optimal hyperparameters\n",
                "\n",
                "### Key Point:\n",
                "\"Decision trees are prone to overfitting without constraints. Pre-pruning using max_depth, min_samples_split, and min_samples_leaf prevents trees from memorizing training data. The key is finding the sweet spot: complex enough to capture patterns, simple enough to generalize. Use cross-validation to tune hyperparameters and always monitor the train-test gap.\"\n",
                "\n",
                "---\n",
                "\n",
                "**Decision Trees component complete!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}