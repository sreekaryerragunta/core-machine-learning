{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Tree From Scratch\n",
                "\n",
                "Building a Decision Tree classifier using only NumPy:\n",
                "1. Implement tree node structure\n",
                "2. Recursive tree building with CART algorithm\n",
                "3. Prediction logic\n",
                "4. Comparison with sklearn\n",
                "5. Visualization\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import load_iris, make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "from utils import (\n",
                "    entropy, gini_impurity, information_gain,\n",
                "    find_best_split, most_common_label, accuracy_score, confusion_matrix\n",
                ")\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Tree Node Structure\n",
                "\n",
                "Define nodes for the decision tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Node:\n",
                "    \"\"\"\n",
                "    Represents a node in the decision tree.\n",
                "    \"\"\"\n",
                "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
                "        # Decision node properties\n",
                "        self.feature = feature        # Index of feature to split on\n",
                "        self.threshold = threshold    # Threshold value for split\n",
                "        self.left = left             # Left subtree\n",
                "        self.right = right           # Right subtree\n",
                "        \n",
                "        # Leaf node property\n",
                "        self.value = value           # Class label (for leaf nodes)\n",
                "    \n",
                "    def is_leaf(self):\n",
                "        \"\"\"Check if node is a leaf.\"\"\"\n",
                "        return self.value is not None\n",
                "\n",
                "print('Node structure defined')\n",
                "print('Decision node: feature, threshold, left, right')\n",
                "print('Leaf node: value (predicted class)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Decision Tree Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecisionTree:\n",
                "    \"\"\"\n",
                "    Decision Tree Classifier implemented from scratch.\n",
                "    \n",
                "    Uses CART algorithm with binary splits.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, max_depth=None, min_samples_split=2, \n",
                "                 min_samples_leaf=1, criterion='gini'):\n",
                "        \"\"\"\n",
                "        Parameters:\n",
                "        -----------\n",
                "        max_depth : int or None\n",
                "            Maximum depth of tree\n",
                "        min_samples_split : int\n",
                "            Minimum samples required to split a node\n",
                "        min_samples_leaf : int\n",
                "            Minimum samples required in a leaf node\n",
                "        criterion : str\n",
                "            'gini' or 'entropy'\n",
                "        \"\"\"\n",
                "        self.max_depth = max_depth\n",
                "        self.min_samples_split = min_samples_split\n",
                "        self.min_samples_leaf = min_samples_leaf\n",
                "        self.criterion = criterion\n",
                "        self.root = None\n",
                "        self.n_features = None\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"\n",
                "        Build the decision tree.\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : ndarray of shape (n_samples, n_features)\n",
                "            Training data\n",
                "        y : ndarray of shape (n_samples,)\n",
                "            Target labels\n",
                "        \"\"\"\n",
                "        self.n_features = X.shape[1]\n",
                "        self.root = self._build_tree(X, y, depth=0)\n",
                "        return self\n",
                "    \n",
                "    def _build_tree(self, X, y, depth):\n",
                "        \"\"\"\n",
                "        Recursively build the tree using CART algorithm.\n",
                "        \"\"\"\n",
                "        n_samples, n_features = X.shape\n",
                "        n_classes = len(np.unique(y))\n",
                "        \n",
                "        # Stopping criteria\n",
                "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
                "           n_classes == 1 or \\\n",
                "           n_samples < self.min_samples_split:\n",
                "            # Create leaf node\n",
                "            leaf_value = most_common_label(y)\n",
                "            return Node(value=leaf_value)\n",
                "        \n",
                "        # Find best split\n",
                "        best_feature, best_threshold = self._find_best_split(X, y)\n",
                "        \n",
                "        if best_feature is None:\n",
                "            # No good split found\n",
                "            leaf_value = most_common_label(y)\n",
                "            return Node(value=leaf_value)\n",
                "        \n",
                "        # Split data\n",
                "        left_indices = X[:, best_feature] <= best_threshold\n",
                "        right_indices = ~left_indices\n",
                "        \n",
                "        # Check minimum samples in leaf\n",
                "        if np.sum(left_indices) < self.min_samples_leaf or \\\n",
                "           np.sum(right_indices) < self.min_samples_leaf:\n",
                "            leaf_value = most_common_label(y)\n",
                "            return Node(value=leaf_value)\n",
                "        \n",
                "        # Recursively build left and right subtrees\n",
                "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
                "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
                "        \n",
                "        # Create decision node\n",
                "        return Node(feature=best_feature, threshold=best_threshold,\n",
                "                   left=left_subtree, right=right_subtree)\n",
                "    \n",
                "    def _find_best_split(self, X, y):\n",
                "        \"\"\"\n",
                "        Find the best feature and threshold to split on.\n",
                "        \n",
                "        Returns:\n",
                "        --------\n",
                "        tuple : (best_feature_idx, best_threshold)\n",
                "        \"\"\"\n",
                "        best_gain = 0\n",
                "        best_feature = None\n",
                "        best_threshold = None\n",
                "        \n",
                "        # Try each feature\n",
                "        for feature_idx in range(self.n_features):\n",
                "            threshold, gain = find_best_split(X, y, feature_idx, self.criterion)\n",
                "            \n",
                "            if gain > best_gain:\n",
                "                best_gain = gain\n",
                "                best_feature = feature_idx\n",
                "                best_threshold = threshold\n",
                "        \n",
                "        return best_feature, best_threshold\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"\n",
                "        Predict classes for samples in X.\n",
                "        \n",
                "        Parameters:\n",
                "        -----------\n",
                "        X : ndarray of shape (n_samples, n_features)\n",
                "            Samples to predict\n",
                "            \n",
                "        Returns:\n",
                "        --------\n",
                "        ndarray : Predicted class labels\n",
                "        \"\"\"\n",
                "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
                "    \n",
                "    def_traverse_tree(self, x, node):\n",
                "        \"\"\"\n",
                "        Traverse tree to make prediction for a single sample.\n",
                "        \"\"\"\n",
                "        if node.is_leaf():\n",
                "            return node.value\n",
                "        \n",
                "        if x[node.feature] <= node.threshold:\n",
                "            return self._traverse_tree(x, node.left)\n",
                "        else:\n",
                "            return self._traverse_tree(x, node.right)\n",
                "    \n",
                "    def get_depth(self):\n",
                "        \"\"\"Get the actual depth of the tree.\"\"\"\n",
                "        def _depth(node):\n",
                "            if node is None or node.is_leaf():\n",
                "                return 0\n",
                "            return 1 + max(_depth(node.left), _depth(node.right))\n",
                "        return _depth(self.root)\n",
                "    \n",
                "    def count_leaves(self):\n",
                "        \"\"\"Count number of leaf nodes.\"\"\"\n",
                "        def _count(node):\n",
                "            if node is None:\n",
                "                return 0\n",
                "            if node.is_leaf():\n",
                "                return 1\n",
                "            return _count(node.left) + _count(node.right)\n",
                "        return _count(self.root)\n",
                "\n",
                "print('Decision Tree class implemented')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Test on Iris Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Iris dataset\n",
                "iris = load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f'Training set: {X_train.shape}')\n",
                "print(f'Test set: {X_test.shape}')\n",
                "print(f'Features: {iris.feature_names}')\n",
                "print(f'Classes: {iris.target_names}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train Our Decision Tree"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train our tree\n",
                "our_tree = DecisionTree(max_depth=5, min_samples_split=2, criterion='gini')\n",
                "our_tree.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = our_tree.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "cm = confusion_matrix(y_test, y_pred, n_classes=3)\n",
                "\n",
                "print('='*60)\n",
                "print('OUR DECISION TREE')\n",
                "print('='*60)\n",
                "print(f'Tree depth: {our_tree.get_depth()}')\n",
                "print(f'Number of leaves: {our_tree.count_leaves()}')\n",
                "print(f'\\nTest Accuracy: {accuracy*100:.2f}%')\n",
                "print(f'\\nConfusion Matrix:\\n{cm}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compare with Scikit-Learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train sklearn tree\n",
                "sklearn_tree = DecisionTreeClassifier(max_depth=5, min_samples_split=2, \n",
                "                                      criterion='gini', random_state=42)\n",
                "sklearn_tree.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
                "cm_sklearn = confusion_matrix(y_test, y_pred_sklearn, n_classes=3)\n",
                "\n",
                "print('='*60)\n",
                "print('SKLEARN DECISION TREE')\n",
                "print('='*60)\n",
                "print(f'Tree depth: {sklearn_tree.get_depth()}')\n",
                "print(f'Number of leaves: {sklearn_tree.get_n_leaves()}')\n",
                "print(f'\\nTest Accuracy: {accuracy_sklearn*100:.2f}%')\n",
                "print(f'\\nConfusion Matrix:\\n{cm_sklearn}')\n",
                "\n",
                "print('\\n' + '='*60)\n",
                "print('COMPARISON')\n",
                "print('='*60)\n",
                "print(f'Our accuracy:     {accuracy*100:.2f}%')\n",
                "print(f'Sklearn accuracy: {accuracy_sklearn*100:.2f}%')\n",
                "print(f'Difference:       {abs(accuracy - accuracy_sklearn)*100:.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4:DecisionBoundary Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create simple 2D dataset\n",
                "X_2d, y_2d = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
                "                                 n_informative=2, n_clusters_per_class=1,\n",
                "                                 random_state=42)\n",
                "\n",
                "# Train tree\n",
                "tree_2d = DecisionTree(max_depth=5, criterion='gini')\n",
                "tree_2d.fit(X_2d, y_2d)\n",
                "\n",
                "# Create mesh for decision boundary\n",
                "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
                "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
                "                     np.arange(y_min, y_max, 0.02))\n",
                "\n",
                "# Predict on mesh\n",
                "Z = tree_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Z = Z.reshape(xx.shape)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 7))\n",
                "plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
                "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, s=50, \n",
                "           edgecolors='k', cmap='RdYlBu')\n",
                "plt.xlabel('Feature 1', fontsize=12)\n",
                "plt.ylabel('Feature 2', fontsize=12)\n",
                "plt.title('Decision Tree Decision Boundary', fontsize=14, fontweight='bold')\n",
                "plt.colorbar()\n",
                "plt.show()\n",
                "\n",
                "print('Notice the rectangular (axis-parallel) decision boundaries!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 5: Effect of Max Depth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "depths = [1, 2, 3, 5, 10]\n",
                "train_accs = []\n",
                "test_accs = []\n",
                "\n",
                "for depth in depths:\n",
                "    tree = DecisionTree(max_depth=depth, criterion='gini')\n",
                "    tree.fit(X_train, y_train)\n",
                "    \n",
                "    train_pred = tree.predict(X_train)\n",
                "    test_pred = tree.predict(X_test)\n",
                "    \n",
                "    train_accs.append(accuracy_score(y_train, train_pred))\n",
                "    test_accs.append(accuracy_score(y_test, test_pred))\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(depths, train_accs, 'o-', linewidth=2, label='Train Accuracy', markersize=8)\n",
                "plt.plot(depths, test_accs, 's-', linewidth=2, label='Test Accuracy', markersize=8)\n",
                "plt.xlabel('Max Depth', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('Effect of Max Depth on Performance', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print('Observation: Deeper trees may overfit (train >> test accuracy)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### What We Built:\n",
                "1. **Node structure** for tree representation\n",
                "2. **Recursive CART algorithm** for tree construction\n",
                "3. **Prediction logic** by traversing the tree\n",
                "4. **Hyperparameters**: max_depth, min_samples_split, min_samples_leaf\n",
                "\n",
                "### Key Insights:\n",
                "- Decision trees create **axis-parallel boundaries** (rectangles)\n",
                "- **Deeper trees** can overfit (memorize training data)\n",
                "- Our implementation **matches sklearn** performance\n",
                "- Trees are **interpretable** - can visualize decision logic\n",
                "\n",
                "### Key Point:\n",
                "\"Decision trees recursively split data using the feature that provides maximum information gain. The CART algorithm creates binary splits at each node, choosing thresholds that minimize impurity (Gini or entropy). Deeper trees fit training data better but may overfit - use max_depth and min_samples constraints to prevent this.\"\n",
                "\n",
                "---\n",
                "\n",
                "**Next**: See pruning_and_overfitting.ipynb for advanced techniques!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}