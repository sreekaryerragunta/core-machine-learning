{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Random Forest From Scratch\n",
                "\n",
                "Building a Random Forest using Decision Trees:\n",
                "1. Implement DecisionTree class\n",
                "2. Bootstrap sampling\n",
                "3. Random Forest ensemble\n",
                "4. sklearn comparison\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "from decision_trees.utils import (\n",
                "    entropy, gini_impurity, information_gain, find_best_split, \n",
                "    most_common_label, accuracy_score, confusion_matrix\n",
                ")\n",
                "from utils import bootstrap_sample, majority_vote\n",
                "\n",
                "sns.set_style('darkgrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Decision Tree (from previous component)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Node:\n",
                "    \"\"\"Node in decision tree.\"\"\"\n",
                "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
                "        self.feature = feature\n",
                "        self.threshold = threshold  \n",
                "        self.left = left\n",
                "        self.right = right\n",
                "        self.value = value\n",
                "    \n",
                "    def is_leaf(self):\n",
                "        return self.value is not None\n",
                "\n",
                "class DecisionTree:\n",
                "    \"\"\"Decision Tree Classifier.\"\"\"\n",
                "    \n",
                "    def __init__(self, max_depth=None, min_samples_split=2, \n",
                "                 min_samples_leaf=1, criterion='gini'):\n",
                "        self.max_depth = max_depth\n",
                "        self.min_samples_split = min_samples_split\n",
                "        self.min_samples_leaf = min_samples_leaf\n",
                "        self.criterion = criterion\n",
                "        self.root = None\n",
                "        self.n_features = None\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        self.n_features = X.shape[1]\n",
                "        self.root = self._build_tree(X, y, depth=0)\n",
                "        return self\n",
                "    \n",
                "    def _build_tree(self, X, y, depth):\n",
                "        n_samples, n_features = X.shape\n",
                "        n_classes = len(np.unique(y))\n",
                "        \n",
                "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
                "           n_classes == 1 or n_samples < self.min_samples_split:\n",
                "            return Node(value=most_common_label(y))\n",
                "        \n",
                "        best_feature, best_threshold = self._find_best_split(X, y)\n",
                "        \n",
                "        if best_feature is None:\n",
                "            return Node(value=most_common_label(y))\n",
                "        \n",
                "        left_indices = X[:, best_feature] <= best_threshold\n",
                "        right_indices = ~left_indices\n",
                "        \n",
                "        if np.sum(left_indices) < self.min_samples_leaf or \\\n",
                "           np.sum(right_indices) < self.min_samples_leaf:\n",
                "            return Node(value=most_common_label(y))\n",
                "        \n",
                "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
                "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
                "        \n",
                "        return Node(feature=best_feature, threshold=best_threshold,\n",
                "                   left=left_subtree, right=right_subtree)\n",
                "    \n",
                "    def _find_best_split(self, X, y):\n",
                "        best_gain = 0\n",
                "        best_feature = None\n",
                "        best_threshold = None\n",
                "        \n",
                "        for feature_idx in range(self.n_features):\n",
                "            threshold, gain = find_best_split(X, y, feature_idx, self.criterion)\n",
                "            if gain > best_gain:\n",
                "                best_gain = gain\n",
                "                best_feature = feature_idx\n",
                "                best_threshold = threshold\n",
                "        \n",
                "        return best_feature, best_threshold\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
                "    \n",
                "    def _traverse_tree(self, x, node):\n",
                "        if node.is_leaf():\n",
                "            return node.value\n",
                "        if x[node.feature] <= node.threshold:\n",
                "            return self._traverse_tree(x, node.left)\n",
                "        else:\n",
                "            return self._traverse_tree(x, node.right)\n",
                "\n",
                "print('Decision Tree class ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Random Forest Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RandomForest:\n",
                "    \"\"\"Random Forest Classifier.\"\"\"\n",
                "    \n",
                "    def __init__(self, n_estimators=100, max_depth=None,\n",
                "                 min_samples_split=2, random_state=None):\n",
                "        self.n_estimators = n_estimators\n",
                "        self.max_depth = max_depth\n",
                "        self.min_samples_split = min_samples_split\n",
                "        self.random_state = random_state\n",
                "        self.trees = []\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        self.trees = []\n",
                "        \n",
                "        for i in range(self.n_estimators):\n",
                "            # Bootstrap sample\n",
                "            seed = self.random_state + i if self.random_state else None\n",
                "            X_sample, y_sample = bootstrap_sample(X, y, random_state=seed)\n",
                "            \n",
                "            # Train tree\n",
                "            tree = DecisionTree(\n",
                "                max_depth=self.max_depth,\n",
                "                min_samples_split=self.min_samples_split\n",
                "            )\n",
                "            tree.fit(X_sample, y_sample)\n",
                "            self.trees.append(tree)\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def predict(self, X):\n",
                "        tree_predictions = [tree.predict(X) for tree in self.trees]\n",
                "        return majority_vote(tree_predictions)\n",
                "\n",
                "print('Random Forest class ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Train on Iris"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "iris = load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Our Random Forest\n",
                "rf = RandomForest(n_estimators=10, max_depth=5, random_state=42)\n",
                "rf.fit(X_train, y_train)\n",
                "y_pred = rf.predict(X_test)\n",
                "\n",
                "acc = accuracy_score(y_test, y_pred)\n",
                "print('='*50)\n",
                "print('OUR RANDOM FOREST')\n",
                "print('='*50)\n",
                "print(f'Trees: {rf.n_estimators}')\n",
                "print(f'Accuracy: {acc*100:.2f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sklearn Random Forest  \n",
                "sklearn_rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
                "sklearn_rf.fit(X_train, y_train)\n",
                "y_pred_sk = sklearn_rf.predict(X_test)\n",
                "\n",
                "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
                "print('='*50)\n",
                "print('SKLEARN RANDOM FOREST')\n",
                "print('='*50)\n",
                "print(f'Trees: {sklearn_rf.n_estimators}')\n",
                "print(f'Accuracy: {acc_sk*100:.2f}%')\n",
                "\n",
                "print(f'\\nDifference: {abs(acc - acc_sk)*100:.2f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 4: Effect of Number of Trees"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_trees_range = [1, 5, 10, 25, 50]\n",
                "accuracies = []\n",
                "\n",
                "for n in n_trees_range:\n",
                "    rf_temp = RandomForest(n_estimators=n, max_depth=5, random_state=42)\n",
                "    rf_temp.fit(X_train, y_train)\n",
                "    pred = rf_temp.predict(X_test)\n",
                "    accuracies.append(accuracy_score(y_test, pred))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(n_trees_range, accuracies, 'o-', linewidth=2, markersize=8)\n",
                "plt.xlabel('Number of Trees', fontsize=12)\n",
                "plt.ylabel('Accuracy', fontsize=12)\n",
                "plt.title('Accuracy vs Number of Trees', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f'Best accuracy: {max(accuracies)*100:.2f}% with {n_trees_range[np.argmax(accuracies)]} trees')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "### Key Insights:\n",
                "- **Ensemble averaging** reduces variance\n",
                "- **Bootstrap sampling** creates diverse training sets\n",
                "- **More trees** â†’ more stable predictions\n",
                "- Performance matches sklearn\n",
                "\n",
                "### Key Point:\n",
                "\"Random Forests combine multiple decision trees trained on bootstrap samples. Aggregating predictions through majority voting reduces variance and improves generalization without increasing bias.\"\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}